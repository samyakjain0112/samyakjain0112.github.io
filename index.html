<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Samyak Jain</title>
  
  <meta name="author" content="Samyak Jain">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/seal_icon.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Samyak Jain</name>
              </p>

        <!-- Description -->
              <p>I am a research fellow at Microsoft Research India, where I am advised by <a href="https://scholar.google.com/citations?user=w8WJCnkAAAAJ&hl=en">Navin Goyal</a>. Before joining MSR, I did research internships with <a href="https://www.kasl.ai/">David Krueger</a> and <a href="https://puneetkdokania.github.io/"></a>Puneet Dokania</a> at Cambridge University and <a href="https://www.five.ai/research">Five AI</a>, where my work focussed on developing a better understanding of (safety) fine-tuning using mechanistic interpretability. Even before this, I completed my Bachelors and Masters in CSE at <a href="https://www.iitbhu.ac.in/">Indian Institute of Technology (BHU) Varanasi</a> and during this time, I worked with <a href="https://val.cds.iisc.ac.in/">Venkatesh Babu</a> and <a href="https://scholar.google.com/citations?user=MOO12i0AAAAJ&hl=en">Sravanti Addepalli</a> at <a href="https://iisc.ac.in/">Indian Institute of Science, Bangalore</a>. Here, my research foccussed on developing adversarially robust defences.
              </p>
              <p>
                I am very interested in research areas related to understanding the inductive biases of the learning algorithms, which can help us explain different intriguing observations: phase transitions, neural collapse, simplicity bias, etc. For this I aim to combine tools from learning theory and mechanistic interpretability. I believe in using toy setups to develop hypotheses explaining real world observations and later test these hypoteses at scale. I am always motivated to deliver impactful work enhancing our scientific understanding while being useful in practice. 
              </p>
              <p>
                I am also interested in domains related to AI Safety, like cooperative alignment, adversarial robustness, reward hacking and safety fine-tuning.
              </p>
        <!-- Stuff -->
              <p style="text-align:center">
                <a href="mailto:samyak0112@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/samyak-cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.pl/citations?user=aKCRSv0AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/samyakjain0112">Github</a>
              </p>
            </td>

        <!-- Front image -->
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/prof_pic.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/prof_pic.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!-- News -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr> <td width="100%" valign="top">
        <heading>News</heading>
        <table width="100%" valign="top" border="0" cellspacing="0" cellpadding="2">


          
        <tr valign="top"> <td> [09/2024] </td>  <td> 
            Paper on <a href="https://arxiv.org/abs/2407.10264">identifying how jailbreaks bypass safety mechanisms</a> accepted at NeurIPS, 2024.
            </td> </tr>

        <!-- <tr valign="top"> <td> [07/2024] </td>  <td> 
            Excited to start as a research fellow at <a href='https://www.microsoft.com/en-us/research/lab/microsoft-research-india/'>Microsoft Research India</a>.
            </td> </tr> -->
            
        <tr valign="top"> <td> [06/2024] </td>  <td> 
        Paper on <a href="https://arxiv.org/abs/2407.10264">identifying how jailbreaks bypass safety mechanisms</a> accepted as <font color="red">spotlight</font> at ICML-MI workshop, 2024.
        </td> </tr>

        <tr valign="top"> <td> [2/2024] </td>  <td> 
          Paper on <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jain_Towards_Understanding_and_Improving_Adversarial_Robustness_of_Vision_Transformers_CVPR_2024_paper.pdf">highlighting cause of gradient masking in vision transformers</a> accepted to CVPR, 2024. 
          </td> </tr>
          
        <tr valign="top"> <td> [01/2024] </td>  <td> 
        Paper on <a href="https://arxiv.org/abs/2311.12786">mechanistically analyzing effects of fine-tuning</a> accepted to ICLR, 2024. 
        </td> </tr>

          <tr valign="top"> <td> [3/2023] </td>  <td> 
            Paper on <a href="https://arxiv.org/abs/2311.12786"> improving domain generalization via exploring the loss landscape</a> accepted to CVPR, 2023.
              </td> </tr>
<!-- 
          <tr valign="top"> <td> [6/2023] </td>  <td> 
            Excited to start as a research intern at <a href='https://www.kasl.ai/'>Krueger AI safety lab, Cambridge University</a>.
            </td> </tr> -->

            <tr valign="top"> <td> [6/2023] </td>  <td> 
              Glad to be recognized as <a href='https://cvpr2023.thecvf.com/Conferences/2023/OutstandingReviewers'>Outstanding Reviewer</a> (top 250 reviewers) at CVPR 2023.
              </td> </tr>

                <tr valign="top"> <td> [9/2022] </td>  <td> 
                  Paper on <a href="https://arxiv.org/abs/2210.15318"> effectively utilizing augmentations for adversarial training</a> accepted to NeurIPS, 2022.
                    </td> </tr>

                    <tr valign="top"> <td> [8/2022] </td>  <td> 
                      Paper on <a href="https://arxiv.org/abs/2210.09852"> scaling adversarial robustness beyond standard threat models</a> accepted to ECCV, 2022.
                        </td> </tr>

            <tr valign="top"> <td> [7/2022] </td>  <td> 
              Glad to be recognized as <a href='https://cvpr2022.thecvf.com/outstanding-reviewers'>Outstanding Reviewer</a> (top 200 reviewers) at CVPR 2022.
              </td> </tr>

              <tr valign="top"> <td> [4/2022] </td>  <td> 
                Glad to be recognized as <a href='https://iclr.cc/Conferences/2022/Reviewers'>Highlighted Reviewer</a> at ICLR 2022.
                </td> </tr>
<!-- 
                <tr valign="top"> <td> [5/2021] </td>  <td> 
                  Was awarded the <a href="https://www2.daad.de/deutschland/stipendium/datenbank/en/21148-scholarship-database/?detail=50015295">DAAD WISE</a> Scholarship for a research intern at the Tecnical University of Munich.
                    </td> </tr>

                    <tr valign="top"> <td> [5/2020] </td>  <td> 
                      Started a research intern at <a href="https://val.cds.iisc.ac.in/"></a>Vision and AI lab</a> at Indian Institute of Science.
                        </td> </tr> -->
        </table>
        </td>  </tr> </tbody></table>


        <!-- Publications -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications (* denotes equal contribution)</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sftjailbreaks.png" alt="SFT and Jailbreaks" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2407.10264">
                <papertitle>What Makes and Breaks Safety Fine-tuning? A Mechanistic Study</papertitle>
              </a>
              <br>
              <strong>Samyak Jain</strong>,
              <a href="https://ekdeepslubana.github.io/">Ekdeep Singh Lubana</a>,
              <a href="https://openreview.net/profile?id=~Kemal_Oksuz1">Kemal Oksuz</a>,
              <a href="https://thwjoy.github.io">Tom Joy</a>,
              <a href="https://www.robots.ox.ac.uk/~phst/">Philip H.S. Torr</a>,
              <a href="https://amartya18x.github.io/">Amartya Sanyal</a>, and
              <a href="https://puneetkdokania.github.io">Puneet K. Dokania</a>
              <br>
              <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2024
              <br>
              <em>ICML workshop on Mechanistic Interpretability </em>, 2024 <font color="red"> (Spotlight)</font> 
              <br> 
              <a href="data/sftjailbreaks.bib">bibtex</a> / <a href="https://arxiv.org/abs/2407.10264">arXiv</a>
              <p>We use formal languages as a model system to identify the mechanistic changes induced by safety fine-tuning, and how jailbreaks bypass said mechanisms, verifying our claims on Llama models.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mft.png" alt="Mechanistic fine-tuning" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2311.12786">
                <papertitle>Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks</papertitle>
              </a>
              <br>
              <strong>Samyak Jain<sup>*</sup></strong>,
              <a href="https://robertkirk.github.io">Robert Kirk<sup>*</sup></a>,
              <a href="https://ekdeepslubana.github.io/">Ekdeep Singh<sup>*</sup></a>
              <a href="http://robertdick.org/">Robert P. Dick</a>,
              <a href="https://sites.google.com/view/htanaka/home">Hidenori Tanaka</a>,
              <a href="https://www.egrefen.com">Edward Grefenstette</a>,
              <a href="https://rockt.github.io">Tim Rocktaschel</a>, and
              <a href="https://www.davidscottkrueger.com">David Krueger</a>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2024
              <br> 
              <a href="data/mft.bib">bibtex</a> / <a href="https://arxiv.org/abs/2311.12786">arXiv</a>
              <p>We show fine-tuning leads to learning of minimal transformations of a pretrained model's capabilities, like a "wrapper", by using procedural tasks defined using Tracr, PCFGs, and TinyStories.</p>
            </td>
          </tr>
          
          
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
